{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nKKZqU06Puz",
        "outputId": "e7acf848-96eb-43cf-a6c1-9a4fbaa786f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the dataset: (569, 32)\n",
            " \n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 569 entries, 0 to 568\n",
            "Data columns (total 32 columns):\n",
            " #   Column                   Non-Null Count  Dtype  \n",
            "---  ------                   --------------  -----  \n",
            " 0   id                       569 non-null    int64  \n",
            " 1   diagnosis                569 non-null    object \n",
            " 2   radius_mean              569 non-null    float64\n",
            " 3   texture_mean             569 non-null    float64\n",
            " 4   perimeter_mean           569 non-null    float64\n",
            " 5   area_mean                569 non-null    float64\n",
            " 6   smoothness_mean          569 non-null    float64\n",
            " 7   compactness_mean         569 non-null    float64\n",
            " 8   concavity_mean           569 non-null    float64\n",
            " 9   concave points_mean      569 non-null    float64\n",
            " 10  symmetry_mean            569 non-null    float64\n",
            " 11  fractal_dimension_mean   569 non-null    float64\n",
            " 12  radius_se                569 non-null    float64\n",
            " 13  texture_se               569 non-null    float64\n",
            " 14  perimeter_se             569 non-null    float64\n",
            " 15  area_se                  569 non-null    float64\n",
            " 16  smoothness_se            569 non-null    float64\n",
            " 17  compactness_se           569 non-null    float64\n",
            " 18  concavity_se             569 non-null    float64\n",
            " 19  concave points_se        569 non-null    float64\n",
            " 20  symmetry_se              569 non-null    float64\n",
            " 21  fractal_dimension_se     569 non-null    float64\n",
            " 22  radius_worst             569 non-null    float64\n",
            " 23  texture_worst            569 non-null    float64\n",
            " 24  perimeter_worst          569 non-null    float64\n",
            " 25  area_worst               569 non-null    float64\n",
            " 26  smoothness_worst         569 non-null    float64\n",
            " 27  compactness_worst        569 non-null    float64\n",
            " 28  concavity_worst          569 non-null    float64\n",
            " 29  concave points_worst     569 non-null    float64\n",
            " 30  symmetry_worst           569 non-null    float64\n",
            " 31  fractal_dimension_worst  569 non-null    float64\n",
            "dtypes: float64(30), int64(1), object(1)\n",
            "memory usage: 142.4+ KB\n",
            "None\n",
            " \n",
            "Number of Benign (B) cases: 357\n",
            "Number of Malignant (M) cases: 212\n",
            " \n",
            "-------------------------------------------------------------------\n",
            "After removing the 'id' column, this is the information of the data\n",
            "-------------------------------------------------------------------\n",
            " \n",
            "Shape of the dataset: (569, 31)\n",
            " \n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 569 entries, 0 to 568\n",
            "Data columns (total 31 columns):\n",
            " #   Column                   Non-Null Count  Dtype  \n",
            "---  ------                   --------------  -----  \n",
            " 0   diagnosis                569 non-null    int64  \n",
            " 1   radius_mean              569 non-null    float64\n",
            " 2   texture_mean             569 non-null    float64\n",
            " 3   perimeter_mean           569 non-null    float64\n",
            " 4   area_mean                569 non-null    float64\n",
            " 5   smoothness_mean          569 non-null    float64\n",
            " 6   compactness_mean         569 non-null    float64\n",
            " 7   concavity_mean           569 non-null    float64\n",
            " 8   concave points_mean      569 non-null    float64\n",
            " 9   symmetry_mean            569 non-null    float64\n",
            " 10  fractal_dimension_mean   569 non-null    float64\n",
            " 11  radius_se                569 non-null    float64\n",
            " 12  texture_se               569 non-null    float64\n",
            " 13  perimeter_se             569 non-null    float64\n",
            " 14  area_se                  569 non-null    float64\n",
            " 15  smoothness_se            569 non-null    float64\n",
            " 16  compactness_se           569 non-null    float64\n",
            " 17  concavity_se             569 non-null    float64\n",
            " 18  concave points_se        569 non-null    float64\n",
            " 19  symmetry_se              569 non-null    float64\n",
            " 20  fractal_dimension_se     569 non-null    float64\n",
            " 21  radius_worst             569 non-null    float64\n",
            " 22  texture_worst            569 non-null    float64\n",
            " 23  perimeter_worst          569 non-null    float64\n",
            " 24  area_worst               569 non-null    float64\n",
            " 25  smoothness_worst         569 non-null    float64\n",
            " 26  compactness_worst        569 non-null    float64\n",
            " 27  concavity_worst          569 non-null    float64\n",
            " 28  concave points_worst     569 non-null    float64\n",
            " 29  symmetry_worst           569 non-null    float64\n",
            " 30  fractal_dimension_worst  569 non-null    float64\n",
            "dtypes: float64(30), int64(1)\n",
            "memory usage: 137.9 KB\n",
            "None\n",
            " \n",
            "---------------------------------\n",
            "We replaced 'B' by 0 and 'M' by 1\n",
            "---------------------------------\n",
            " \n",
            "Number of Benign (B) cases: 357\n",
            "Number of Malignant (M) cases: 212\n",
            " \n",
            "--------------------------------------------------------\n",
            "We divided the dataset into training set and testing set\n",
            "--------------------------------------------------------\n",
            " \n",
            "(455, 31)\n",
            "(114, 31)\n",
            " \n",
            "Error: 0.026440349900673326 at epoch 1 \n",
            "Error: 0.0027030212959951454 at epoch 2 \n",
            "Error: 0.0014526608522533245 at epoch 3 \n",
            "Error: 0.0009856098975476045 at epoch 4 \n",
            "Error: 0.0007423448057715223 at epoch 5 \n",
            " \n",
            "The accuracy is 100.0000 %\n",
            " \n",
            "The number of values mispredicted is: \n",
            " (0, 1) \n",
            " [0, 0]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pickle import TRUE\n",
        "from random import random\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "PRED_ERROR = 0.1\n",
        "\n",
        "class Neural_Network:\n",
        "\n",
        "    # initialize the neural network\n",
        "    def __init__(self, num_inputs, num_hidden , num_outputs):\n",
        "        self.num_inputs = num_inputs   # num_inputs = number of nodes in the input layer\n",
        "        self.num_hidden = num_hidden   # num_hidden = number of nodes in the hidden layers i.e: hidden_layers_size [20, 30] it has 2 hidden layers first hidden layer has\n",
        "                                       # 20 nodes and the second hidden layer has 30 nodes\n",
        "        self.num_outputs = num_outputs # num_outputs = number of nodes in the output layer\n",
        "        layers = [self.num_inputs] + self.num_hidden + [self.num_outputs] # obtain a list that represents the size of each layer in the neural network.\n",
        "                                       # i.e: self.num_inputs = 4, self.num_hidden = [8, 16], and self.num_outputs = 3, then layers will be equal to [4, 8, 16, 3]\n",
        "\n",
        "        # initiate weights\n",
        "        self.weights = []      # an empty weights list\n",
        "        self.prev_weights = [] # storing previous weights list\n",
        "        self.momentumterm = []\n",
        "\n",
        "        # initialize the weights randomly\n",
        "        for i in range(len(layers) - 1):\n",
        "            w = np.random.rand(layers[i] , layers[i+1]) # list of weight matrices which has rows and columns to the corresponding layer and the one after it\n",
        "            self.weights.append(w)\n",
        "\n",
        "        activations = []             # empty list created to store the activations (outputs) of each layer of the neural network during the forward propagation\n",
        "        for i in range(len(layers)): # iterating through layers and storing them in the activation list\n",
        "            a = np.zeros(layers[i])\n",
        "            activations.append(a)\n",
        "        self.activations = activations\n",
        "\n",
        "        derivatives = [] # list to store the derivatives of the weights between each layer during backpropagation\n",
        "        for i in range(len(layers)-1):\n",
        "            d = np.zeros((layers[i], layers[i+1]))\n",
        "            derivatives.append(d)\n",
        "        self.derivatives = derivatives\n",
        "\n",
        "    def feedforward(self, inputs):\n",
        "        activations = inputs # Set the input layer activations to the input values\n",
        "        self.activations[0]= inputs # Store the input layer activations in self.activations[0]\n",
        "        for i, w in enumerate(self.weights): # Iterate over the weights while keeping track of the index\n",
        "            # Calculate net inputs to each layer by taking the dot product of the activations and weights for that layer\n",
        "            net_inputs = np.dot(activations, w)\n",
        "            # Calculate the activation of the layer using the sigmoid activation function\n",
        "            activations = self.sigmoid(net_inputs)\n",
        "            # Store the activations for each layer in self.activations\n",
        "            self.activations[i+1] = activations\n",
        "        return activations\n",
        "\n",
        "    def backpropagation(self, error, verbose = False):\n",
        "        # Loop over the layers in reverse order\n",
        "        for i in reversed(range(len(self.derivatives))):\n",
        "            # Get the activations for the current layer\n",
        "            activations = self.activations[i+1]\n",
        "            # Calculate delta, the error multiplied by the derivative of the activation function\n",
        "            delta = error * self.sigmoid_derivative(activations)\n",
        "            # Reshape delta to have dimensions (n, 1) for later dot product operation\n",
        "            delta_reshaped = delta.reshape(delta.shape[0], -1).T\n",
        "            # Get the activations for the previous layer\n",
        "            current_activations = self.activations[i]\n",
        "            # Reshape activations to have dimensions (n, 1) for later dot product operation\n",
        "            current_activations_reshaped = current_activations.reshape(current_activations.shape[0], -1)\n",
        "            # Calculate the derivative for the current layer\n",
        "            self.derivatives[i] = np.dot(current_activations_reshaped, delta_reshaped)\n",
        "            # Calculate the error for the previous layer using the weights and delta for the current layer\n",
        "            error = np.dot(delta, self.weights[i].T)\n",
        "\n",
        "            #verbose is a boolean parameter that is used to control whether or not to print out intermediate results during the backpropagation process\n",
        "            if verbose:\n",
        "                print(\"Derivatives for W{}: {}\".format(i, self.derivatives[i]))\n",
        "\n",
        "            # Return the error for the input layer\n",
        "        return error\n",
        "\n",
        "   # Update the weights\n",
        "    def update_weights(self, learning_rate):\n",
        "        for i in range(len(self.weights)):\n",
        "            weights = self.weights[i] # Get the current weights\n",
        "            derivatives = self.derivatives[i] # Get the derivatives of the current layer\n",
        "            momentum = self.momentumterm[i]\n",
        "            weights += momentum + derivatives * learning_rate # Update the weights with the momentum and the derivatives multiplied by the learning rate\n",
        "\n",
        "    def momentum_fct(self, momentum):\n",
        "        self.saveprev_weights(self.weights)\n",
        "        for i in range(len(self.weights)):\n",
        "            weights = self.weights [i]\n",
        "            savedprev_weights = self.prev_weights[i]\n",
        "            x = momentum * ( weights - savedprev_weights )\n",
        "            self.momentumterm.append(x)\n",
        "\n",
        "    # Save the current weights of the neural network to the prev_weights\n",
        "    def saveprev_weights(self, currentweights):\n",
        "        self.prev_weights.clear()\n",
        "        for i in range(len(currentweights)):\n",
        "            self.prev_weights.append(currentweights[i])\n",
        "\n",
        "    def train(self, inputs, targets, epochs, learning_rate, momentum, verbose = False):\n",
        "\n",
        "        for i in range(epochs):\n",
        "            sum_error = 0\n",
        "            for input, target in zip(inputs, targets):\n",
        "\n",
        "                output = self.feedforward(input)\n",
        "\n",
        "                #calculate error\n",
        "                error = (target - output) # desired - actual output\n",
        "\n",
        "                #back propagation\n",
        "                self.backpropagation(error)\n",
        "\n",
        "                #apply momentum formula to update momentum\n",
        "                self.momentum_fct(momentum)\n",
        "\n",
        "                #apply gradient descent\n",
        "                self.update_weights(learning_rate)\n",
        "\n",
        "                sum_error += self.mse_fct(target, output)\n",
        "            #report error\n",
        "            if verbose:\n",
        "                print(\"Error: {} at epoch {} \".format(sum_error / len(inputs), i+1))\n",
        "\n",
        "    def predict(self, testing_set, testing_output_set):\n",
        "        predictionTable = []\n",
        "        predictionTable1 = []\n",
        "        x = 0\n",
        "        y = 0\n",
        "        x1 = 0\n",
        "        y1 = 0\n",
        "        for i , target in enumerate(testing_output_set):\n",
        "            output = self.feedforward(testing_set)\n",
        "            error = self.mse_fct(target , output[i])\n",
        "\n",
        "            if error <= PRED_ERROR:\n",
        "                x += 1\n",
        "            else:\n",
        "                y += 1\n",
        "                if np.round_(output[i]) == 0:\n",
        "                    x1 += 1\n",
        "                else:\n",
        "                    y1 += 1\n",
        "            predictionTable = [x, y]\n",
        "            predictionTable1 = [x1, y1]\n",
        "        return predictionTable , predictionTable1\n",
        "\n",
        "    def mse_fct(self, target, output):\n",
        "        return np.average((target-output)**2)\n",
        "\n",
        "    def sigmoid_derivative(self, x):\n",
        "        return x * (1 - x)\n",
        "\n",
        "    def sigmoid(self,x):\n",
        "        return (1 / (1 + np.exp(-x)))\n",
        "\n",
        "    def normalize(a):\n",
        "        for i in range(a.shape[0]):\n",
        "            for j in range(a.shape[1]):\n",
        "                a[i,j] = (a[i,j] - a.min())/(a.max() - a.min())\n",
        "        return a\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    data = pd.read_csv(r\"/content/sample_data/wisc_bc_data.csv\")\n",
        "    # Display the number of rows and columns\n",
        "    print(\"Shape of the dataset:\", data.shape)\n",
        "    print(\" \")\n",
        "\n",
        "    # Display data types and non-null counts\n",
        "    print(data.info())\n",
        "\n",
        "    # Assuming your DataFrame is named 'data'\n",
        "    diagnosis_counts = data['diagnosis'].value_counts()\n",
        "\n",
        "    print(\" \")\n",
        "\n",
        "    print(\"Number of Benign (B) cases:\", diagnosis_counts['B'])\n",
        "    print(\"Number of Malignant (M) cases:\", diagnosis_counts['M'])\n",
        "\n",
        "    data = data.drop('id', axis=1)\n",
        "    data['diagnosis'].replace({'M': 1, 'B': 0}, inplace=True)\n",
        "\n",
        "    print(\" \")\n",
        "    print(\"-------------------------------------------------------------------\")\n",
        "    print(\"After removing the 'id' column, this is the information of the data\")\n",
        "    print(\"-------------------------------------------------------------------\")\n",
        "    print(\" \")\n",
        "\n",
        "    # Display the number of rows and columns\n",
        "    print(\"Shape of the dataset:\", data.shape)\n",
        "    print(\" \")\n",
        "\n",
        "    # Display data types and non-null counts\n",
        "    print(data.info())\n",
        "\n",
        "    # Assuming your DataFrame is named 'data'\n",
        "    diagnosis_counts = data['diagnosis'].value_counts()\n",
        "\n",
        "    print(\" \")\n",
        "    print(\"---------------------------------\")\n",
        "    print(\"We replaced 'B' by 0 and 'M' by 1\")\n",
        "    print(\"---------------------------------\")\n",
        "    print(\" \")\n",
        "\n",
        "    print(\"Number of Benign (B) cases:\", diagnosis_counts[0])\n",
        "    print(\"Number of Malignant (M) cases:\", diagnosis_counts[1])\n",
        "\n",
        "    print(\" \")\n",
        "    print(\"--------------------------------------------------------\")\n",
        "    print(\"We divided the dataset into training set and testing set\")\n",
        "    print(\"--------------------------------------------------------\")\n",
        "    print(\" \")\n",
        "\n",
        "    data = np.array(data)\n",
        "\n",
        "    TRAINING_PERCENTAGE = 80\n",
        "    n_epochs = 5\n",
        "\n",
        "    # Set the number of times the data will be shuffled before each training\n",
        "    n_datashuffle = 1\n",
        "    LearningRate = 0.9\n",
        "    Momentum = 0.6\n",
        "\n",
        "    Neural_Network.normalize(data)\n",
        "\n",
        "    data_training =(data[:int(TRAINING_PERCENTAGE/100 * data.shape[0])])\n",
        "    data_testing =(data[int(TRAINING_PERCENTAGE/100 * data.shape[0]):])\n",
        "\n",
        "    mlp = Neural_Network(30, [1] , 1)\n",
        "\n",
        "    # Shuffle the data and train the neural network multiple times\n",
        "    for i in range(n_datashuffle):\n",
        "\n",
        "        # Print the shapes of the training and validation sets\n",
        "        print(data_training.shape)\n",
        "        print(data_testing.shape)\n",
        "\n",
        "        print(\" \")\n",
        "\n",
        "        # Extract the inputs and outputs from the training and validation sets\n",
        "        trainingSet = data_training[:,:30]\n",
        "        testing_set = data_testing[:,:30]\n",
        "        trainingTargetSet = data_training[:,30]\n",
        "        testing_output_set = data_testing[:,30]\n",
        "\n",
        "        # Train the neural network using the training data\n",
        "        mlp.train(trainingSet, trainingTargetSet, n_epochs, LearningRate , Momentum, TRUE)\n",
        "\n",
        "        # Shuffle the training data for the next iteration\n",
        "        np.random.shuffle(data_training)\n",
        "\n",
        "    # Predict validation set using the trained model\n",
        "    predTable, predTable1 = mlp.predict(testing_set, testing_output_set)\n",
        "\n",
        "    print(\" \")\n",
        "\n",
        "    # Print predicted table and calculate accuracy\n",
        "    # print(predTable)\n",
        "    # print(\" \")\n",
        "\n",
        "    accuracyPercentage = predTable[0]*100/(predTable[0] + predTable[1])\n",
        "    accuracyPercentage=\"%.4f\" % accuracyPercentage\n",
        "    print(\"The accuracy is {} %\".format (accuracyPercentage))\n",
        "    print(\" \")\n",
        "\n",
        "    # Print mispredicted values\n",
        "    print(\"The number of values mispredicted is: \\n (0, 1) \\n\" , predTable1)\n",
        "\n",
        "    # # Plots the training and validation accuracy over epochs.\n",
        "    # plt.plot(accuracyPercentage)\n",
        "    # plt.title(\"Model Accuracy\")\n",
        "    # plt.xlabel('Epoch')\n",
        "    # plt.ylabel('Accuracy')\n",
        "    # plt.legend(['train', 'validation'], loc='best')\n",
        "    # plt.show()\n",
        "\n",
        "    # errors = mlp.train(trainingSet, trainingTargetSet, n_epochs, LearningRate , Momentum, TRUE)\n",
        "    # plt.plot(errors)\n",
        "    # plt.xlabel('Epochs')\n",
        "    # plt.ylabel('Error')\n",
        "    # plt.title('Training Error Curve')\n",
        "    # plt.show()\n",
        "\n",
        "    # plt.scatter(trainingSet[:,0], trainingSet[:,1], c = trainingTargetSet, cmap = 'winter', alpha = 0.5)\n",
        "    # # trainingSet[:,0] and trainingSet[:,1] are arrays of x and y coordinates for the training data points.\n",
        "    # # c = trainingTargetSet is an array of color values corresponding to each data point, and it is used to color the points on the plot.\n",
        "    # plt.scatter(testing_set[:,0], testing_set[:,1], c=testing_output_set, cmap='hot', alpha=0.5)\n",
        "    # # testing_set[:,0] and testing_set[:,1] are arrays of x and y coordinates for the testing data points.\n",
        "    # # c = testing_output_set is an array of color values corresponding to each data point, and it is used to color the points on the plot.\n",
        "    # plt.show() # displays the plot on the screen."
      ]
    }
  ]
}